
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro:ital,wght@0,400;0,700;1,400&family=Source+Sans+Pro:ital,wght@0,300;0,400;0,700;1,400&display=swap" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="/theme/stylesheet/style.min.css">


    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
          href="/theme/pygments/github.min.css">



  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="/theme/font-awesome/css/solid.css">




  <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Test Atom">



 

<meta name="author" content="mwokeefe" />
<meta name="description" content="Introduction Controlling output length in large language models (LLMs) is a persistent challenge, especially in production systems where structured responses or consistent verbosity are essential. Standard prompting techniques and maximum token limits offer only coarse control, often resulting in outputs that are too short, too long, or unnaturally truncated. To …" />
<meta name="keywords" content="">


  <meta property="og:site_name" content="Test"/>
  <meta property="og:title" content="Precise length control for large language models"/>
  <meta property="og:description" content="Introduction Controlling output length in large language models (LLMs) is a persistent challenge, especially in production systems where structured responses or consistent verbosity are essential. Standard prompting techniques and maximum token limits offer only coarse control, often resulting in outputs that are too short, too long, or unnaturally truncated. To …"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="/precise-length-control-for-large-language-models.html"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2020-01-01 00:00:00+00:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="/author/mwokeefe.html">
  <meta property="article:section" content="projects"/>
  <meta property="og:image" content="">

  <title>Test &ndash; Precise length control for large language models</title>


</head>
<body class="light-theme">

<aside>
  <div>
    <a href="/">
      <img src="/theme/img/profile.png" alt="" title="">
    </a>

    <h1>
      <a href="/"></a>
    </h1>



    <nav>
      <ul class="list">


            <li>
              <a target="_self"
                 href="/#home">
                Welcome to My Site
              </a>
            </li>
            <li>
              <a target="_self"
                 href="/pages/projects.html#projects">
                Projects
              </a>
            </li>
            <li>
              <a target="_self"
                 href="/pages/tech.html#tech">
                Tech
              </a>
            </li>
            <li>
              <a target="_self"
                 href="/pages/papers.html#papers">
                Papers
              </a>
            </li>

      </ul>
    </nav>

  </div>

</aside>
  <main>


<article class="single">
  <header>
      
    <h1 id="precise-length-control-for-large-language-models">Precise length control for large language models</h1>
    <p>
      Posted on Wed 01 January 2020 in <a href="/category/projects.html">projects</a>

    </p>
  </header>


  <div>
    <h1>Introduction</h1>
<p>Controlling output length in large language models (LLMs) is a persistent challenge, especially in production systems where structured responses or consistent verbosity are essential. Standard prompting techniques and maximum token limits offer only coarse control, often resulting in outputs that are too short, too long, or unnaturally truncated.</p>
<p>To address this, we introduce a simple but effective method to enable precise response length control in decoder-only transformer architectures using Length-Difference Positional Encoding (LDPE).</p>
<h1>The Core Idea: Countdown Positional Encoding</h1>
<p>LDPE works by injecting an additional positional signal into the model that counts down from a user-defined target response length. Instead of altering the model's existing RoPE or absolute positional encodings — which would likely degrade performance — we add a secondary sinusoidal encoding to the input token embeddings. This reverse positional signal indicates how many tokens remain until the response should terminate.</p>
<p>Two variations were tested:</p>
<p>LDPE: Applies the countdown across the full prompt-response sequence.
ORPE: Offsets the countdown to begin only at the start of the generated response.
We fine-tune open-weight LLMs (e.g., Mistral 7B, LLaMA 3 8B) with this additional countdown signal. During training, the model learns to interpret the countdown as a form of token budget, allowing it to structure responses to end cleanly when the countdown reaches zero.</p>
<h1>Integration and Training</h1>
<p>The reverse encodings are constructed using the standard sinusoidal formula and added directly to the token embeddings. To avoid scale mismatches between the learned embeddings and the sinusoidal signals, we normalize the added encoding using Frobenius norm scaling.</p>
<p>Fine-tuning is performed using LoRA adapters, allowing efficient adaptation without overwriting pretrained weights. We found that a single epoch on a modest-sized dataset (~110k QA samples from OpenOrca and MMLU) was sufficient to achieve strong length control.</p>
<h1>Results: Sub-Token Accuracy Without Quality Loss</h1>
<p>On a QA benchmark with variable-length targets (10–200 tokens), LDPE-fine-tuned models consistently generated outputs within 2–3 tokens of the target length. This level of control was an order of magnitude more accurate than prompt-only fine-tuning approaches.</p>
<p>We also tested summarization on CNN/DailyMail using GPT-3.5-generated summaries as soft references. Summary quality, as measured by BERT and ROUGE scores, was on par with prompt-based baselines, but LDPE models matched the target length 10x more accurately.</p>
<p>To ensure the model’s general capabilities weren’t compromised, we evaluated it on several standard NLP benchmarks (ARC, PIQA, HellaSwag, WINOGRANDE). Task performance was largely preserved after fine-tuning, with no major regressions except a small drop on HellaSwag.</p>
<h1>Takeaways</h1>
<p>LDPE enables fine-grained, token-level control over LLM output length with minimal overhead:</p>
<p>No changes to model architecture
Compatible with any decoder-only LLM
Maintains performance across QA, summarization, and benchmark tasks
Enables smoother response termination and better format compliance
This approach unlocks new capabilities for LLM deployments where deterministic length behavior is critical — including multi-step reasoning chains, format-sensitive APIs, and human-in-the-loop systems.</p>
<h1>Citing articles</h1>
<p>Add...</p>
  </div>
  <div class="tag-cloud">
    <p>
    </p>
  </div>






</article>

<footer>
<p>&copy;  </p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Test ",
  "url" : "",
  "image": "",
  "description": ""
}
</script>
</body>
</html>